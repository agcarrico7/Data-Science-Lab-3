---
title: "Data Science Lab 3"
author: "Amanda Carrico, Hexiuli Huang"
date: "2023-11-08"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: 2
    number_sections: no
    theme: cerulean
    toc_float:
      collapsed: yes
      smooth_scroll: yes
  word_document:
    toc: no
  pdf_document:
    toc: no
---

```{r,echo=FALSE}
knitr::opts_chunk$set(cache=TRUE, echo=TRUE, error=FALSE, message=FALSE, warning=FALSE, tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

<br>

Github link: https://github.com/agcarrico7/Data-Science-Lab-3

<br>

```{r libraries}
# load libraries needed for task
library(tidyverse)
library(fields)
library(amap)
library(knitr)
library(GGally)
```

<br>

## Problem 1: Write your own k-means clustering function. 
### Name your function k_means 
### This function should have three arguments: 
### - data for the dataset to be clustered 
### - k for the number of clusters k
### - iter for the number of iterations performed
### In your function start by randomly assigning each observation to a cluster and calculate the centroids for each of the cluster 
### In the loop (loop through for the number of iterations):
### - calculate the distance from each point to each of the centroids (Hint: I used fields::rdist for this)
### - calculate the minimum distance for each point to the centroids (Hint: I used an apply statement with which.min)
### - calculate the new centroids (and then repeat!)
### Return the cluster assignments for each of the observations in the dataset. Don’t forget to set a seed!
<br>
```{r}
set.seed(2234)

k_means <- function(data, k, iter) {
  ## assign random cluster number to each observation
  cluster_assignments <- sample(1:k, nrow(data), replace = TRUE)
  
  for (i in iter){
    ## calculate centroids by finding means
    centroids <- sapply(1:k, function(y) colMeans(data[cluster_assignments == y, ]))
    
    ## change the format in order to find the correct new cluster group
    centroids <- t(centroids)
    
    ## calculate the distance of each datapoint to its centroids
    distances <- rdist(data, centroids)
    
    ## find the minimum distance and assign to new cluster group
    closest_clusters <- apply(distances, 1, which.min)
    
    ## overwrite the cluster number for each observation
    cluster_assignments <- closest_clusters
  }
  return(list(cluster_assignments,distances))
}
```

<br>

## Problem 2: Cluster the iris dataset using your function k_means
### Hint: Don’t forget to scale the data before you cluster!
### Try this with 3 clusters and 50 iterations. Use GGally:ggpairs to visualize the four variables colored by your cluster assignment. Create a contingency table of our clustering versus the true labels. Comment on your findings.
<br>
```{r}
## prepare dataset
data("iris")
scaled_iris <- iris %>%
  select(-Species) %>%
  scale()
```

```{r}
## clutser the iris dataset
clust_func_return <- k_means(data = scaled_iris, k = 3, iter = 50)
cluster_assignments <- clust_func_return[[1]]

## create new dataset with assigned cluster group
cluster_assignments <- factor(cluster_assignments)
iris_cluster <- cbind(iris, cluster_assignments)

## plots
ggpairs(iris_cluster, columns = 1:4, 
        aes(color = cluster_assignments, alpha = 0.5))

## recreate the random cluster number list
set.seed(2234)
random_labels <- sample(1:3, nrow(iris), replace = TRUE)

## create new dataset with true and random cluster groups
iris_cluster <- cbind(iris_cluster, random_labels)

## create contingency table
contin_table <- table(True = iris_cluster$cluster_assignments, Random = random_labels)
print(contin_table)
```
Since we have four categories in iris dataset, the calculation of 4d distance from each point to each of the centroids is very complicated. Thus, we can notice that the points in group 2 and group 3 are mixed together in most of those 2d dot-plots. On the other hand, 31 of 53 observations in the group 1 that were randomly assign under seed 2234 stay in group 1. 22 of 48 observations in the group 1 that were randomly assign under seed 2234 stay in group 2. 12 of 49 observations in the group 1 that were randomly assign under seed 2234 stay in group 2.

<br>

## Problem 3: Euclidean and Correlation Distance
### Perform hierarchical clustering using (1) Euclidean distance and (2) correlation distance (Hint: you can use amap::Dist() with method = 'correlation') as the dissimilarity measure and the below linkages, and plot the dendrograms for each linkage:
### - complete linkage
### - single linkage
### - average linkage
### Determine clusters for 3 groups from all three methods using cutree(). Create six contingency tables for the results. Comment on what you observe.
```{r 3euclidean}
# scale to observe
scaled_iris <- scale(iris[1:4])
# use euclidean distance and store to cluster later
iris_eu_dist <- dist(scaled_iris)

# plot complete linkage using hclust, distance stored in iris_eu_dist, and plot function - use complete method
eu_complete <- hclust(iris_eu_dist, method = 'complete')
plot(eu_complete, main = "Iris Complete Hierarchical Cluster Dendrogram \n Using Euclidean Distance", hang = -1, cex = .35)

# plot average linkage using hclust, distance stored in iris_eu_dist, and plot function - use average method
eu_average <- hclust(iris_eu_dist, method = 'average')
plot(eu_average, main = "Iris Average Hierarchical Cluster Dendrogram \n Using Euclidean Distance", hang = -1, cex = .35)

# plot single linkage using hclust, distance stored in iris_eu_dist, and plot function - use single method
eu_single <- hclust(iris_eu_dist, method = 'single')
plot(eu_single, main = "Iris Single Hierarchical Cluster Dendrogram \n Using Euclidean Distance", hang = -1, cex = .35)

# clusters for 3 groups using each method, each stored in own variable to use later
eu_comp_3 <- cutree(eu_complete, 3)
eu_avg_3 <- cutree(eu_average, 3)
eu_sing_3 <- cutree(eu_single, 3)
```

```{r 3correlation}
# use correlation distance with amap::dist and store to cluster later
iris_cor_dist <- amap::Dist(scaled_iris, method = 'correlation')

# plot complete linkage using hclust, distance stored in iris_cor_dist, and plot function - use complete method
cor_complete <- hclust(iris_cor_dist, method = 'complete')
plot(cor_complete, main = "Iris Complete Hierarchical Cluster Dendrogram \n Using Correlation Distance", hang = -1, cex = .35, horiz = TRUE)

# plot average linkage using hclust, distance stored in iris_cor_dist, and plot function - use average method
cor_average <- hclust(iris_cor_dist, method = 'average')
plot(cor_complete, main = "Iris Average Hierarchical Cluster Dendrogram \n Using Correlation Distance", hang = -1, cex = .35, horiz = TRUE)

# plot single linkage using hclust, distance stored in iris_cor_dist, and plot function - use single method
cor_single <- hclust(iris_cor_dist, method = 'single')
plot(cor_single, main = "Iris Single Hierarchical Cluster Dendrogram \n Using Correlation Distance", hang = -1, cex = .35, horiz = TRUE)

# clusters for 3 groups using each method, each stored in own variable to use later
cor_comp_3 <- cutree(cor_complete, 3)
cor_avg_3 <- cutree(cor_average, 3)
cor_sing_3 <- cutree(cor_single, 3)
```

```{r contingency_tables_eu}
# store cluster names in a variable to use
names <- c("Cluster 1", "Cluster 2", "Cluster 3")
# use table function to get how many observations in each group, store in dataframe to use later
table_eucomp <- t(as.data.frame(table(eu_comp_3)))[2,]
# use names function and names stored in names variable to add cluster names to data frame
names(table_eucomp) <- names
# create contingency table for Complete Euclidean Clusters using kable function
t(table_eucomp) %>%
  kable(caption = "Iris Complete Clusters \n Using Euclidean Distance")

# use same steps as prior to create contingency table for Average Euclidean Clusters
table_euavg <- t(as.data.frame(table(eu_avg_3)))[2,]
names(table_euavg) <- names
t(table_euavg) %>%
  kable(caption = "Iris Average Clusters \n Using Euclidean Distance")

# same steps as prior to create contingency table for Single Euclidean Clusters
table_eusing <- t(as.data.frame(table(eu_sing_3)))[2,]
names(table_eusing) <- names
t(table_eusing) %>%
  kable(caption = "Iris Single Clusters \n Using Euclidean Distance")
```

```{r contingency_tables_cor}
# create contingency table for Complete Correlation Clusters
table_corcomp <- t(as.data.frame(table(cor_comp_3)))[2,]
names(table_corcomp) <- names
t(table_corcomp) %>%
  kable(caption = "Iris Complete Clusters \n Using Correlation Distance")

# create contingency table for Average Correlation Clusters
table_coravg <- t(as.data.frame(table(cor_avg_3)))[2,]
names(table_coravg) <- names
t(table_coravg) %>%
  kable(caption = "Iris Average Clusters \n Using Correlation Distance")

# create contingency table for Single Correlation Clusters
table_corsing <- t(as.data.frame(table(cor_sing_3)))[2,]
names(table_corsing) <- names
t(table_corsing) %>%
  kable(caption = "Iris Single Clusters \n Using Correlation Distance")
```

Using Euclidean distance vs Correlation distance to determine clusters for 3 groups will give similar distributions by type of linkage. For complete linkage, Euclidean cluster groups contained one with 49, one with 24, and one with 77 while the Correlation cluster 
groups had 51, 25, and 74 observations. Similar observations can be seen with Average and Single linkage. In Average linkage, Euclidean groups contained 50, 97, and 3 per group while Correlation had 51, 94, and 5. Single linkage groups for Euclidean distance had 49, 1, and 100 observations in each and for Correlation, it was 51, 98, and 1.

<br>

## Problem 4: Comparison
### Compare the results from k-means clustering and hierarchical clustering. Which method do you think performed better? Why?
```{r 4}
cat("The within cluster variation for complete correlation hierarchical clustering was", cor_complete$height[147], "\n")
cat("The within cluster variation for average correlation hierarchical clustering was", cor_average$height[147], "\n")
cat("The within cluster variation for single correlation hierarchical clustering was", cor_single$height[147], "\n")

cat("The within cluster variation for complete euclidean hierarchical clustering was", eu_complete$height[147], "\n")
cat("The within cluster variation for average euclidean hierarchical clustering was", eu_average$height[147], "\n")
cat("The within cluster variation for single euclidean hierarchical clustering was", eu_single$height[147], "\n")

distances <- clust_func_return[[2]]
cat("The variances for kmeans clustering: \n")
cat("The variation in the first cluster was", sum(distances[1]^2)/150, "\n")
cat("The variation in the second cluster was", sum(distances[2]^2)/150, "\n")
cat("The variation in the first cluster was", sum(distances[3]^2)/150, "\n")
```
The k-means method to cluster in 3 groups with 50 iterations contained 69 observations in one group, 56 in another, and 25 in the last. The groups appear to be more evenly spread out (there isn't one group that only has 1 observation). In addition, the within cluster variation is a sign of effective clustering when it is low. Out of the hierarchical methods, the single correlation hierarchical clustering had the lowest within cluster variation. However, the clustering method with the highest between cluster variation and lowest within cluster variation was in fact k-means in general. The groups created using k-means clustering were better than any of the Hierarchical methods which makes sense given that k-means clustering is more effective on large data sets.